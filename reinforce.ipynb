{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook imports\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multinomial import Multinomial\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# The policy is a simple linear network with no hidden layers.\n",
    "# Using the softmax, the output is a probability distribution\n",
    "# over the available actions given the current state.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.output = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.softmax(self.output(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "def REINFORCE(env, state_space_size, action_space_size, num_episodes, num_repeats=50, gamma=0.99, disp=print):\n",
    "    \n",
    "    # Check to see if we can run on the GPU\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create policy network (i.e. differentiable policy function)\n",
    "    policy = PolicyNetwork(state_space_size, action_space_size).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Record all of the scores to review later\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        # All states, actions and rewards need to be recorded for training\n",
    "        states  = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Re-seed every few episodes. This gives REINFORCE a few extra shots at the same episode\n",
    "        # so it has time to learn before being overwhelmed by too much randomness.\n",
    "        if i % num_repeats == 0:\n",
    "            seed = np.random.randint(0, np.iinfo(np.int64).max)\n",
    "        env.seed(seed)\n",
    "        \n",
    "        # Reset the score and the environment for this episode\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Play out an episode using the current policy\n",
    "        while True:\n",
    "            \n",
    "            # Take a step and generate action probabilities for current state\n",
    "            # The state must first be turned into a tensor and sent to the device\n",
    "            state_tensor = torch.from_numpy(state).float().to(device)\n",
    "            action_probs = policy.forward(state_tensor)\n",
    "            \n",
    "            # Sample from softmax output to get next action\n",
    "            # 'Categorical' is the same as 'Multinomial'\n",
    "            m = Categorical(action_probs)\n",
    "            action = m.sample()\n",
    "            \n",
    "            # Take another step, update the state, and check the reward\n",
    "            # Calling item retrieves the action value from the action tensor\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            score += reward\n",
    "            \n",
    "            # Record all of our episode stats\n",
    "            rewards.append(reward)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            # Update the state for the next step\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                disp(\"Episode: {} Score: {}\".format(i, score))\n",
    "                break\n",
    "                \n",
    "        # Now that the episode is done, update out policy for each timestep\n",
    "        for t in range(len(states)):\n",
    "\n",
    "            # Get returns at all times, i.e. G[t] for all t\n",
    "            G = sum([r * gamma ** i for i, r in enumerate(rewards[t:])])\n",
    "\n",
    "            # Update our weights. First, zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Convert state to a tensor and re-evaluate probability distribution\n",
    "            state_tensor = torch.from_numpy(states[t]).float().to(device)\n",
    "            probs = policy(state_tensor)\n",
    "            \n",
    "            # Evaluate performanc as per the policy gradient theorem and update our\n",
    "            # weights to take a step in the direction of increased performance.\n",
    "            m = Categorical(probs)\n",
    "            performance = -m.log_prob(actions[t]) * G\n",
    "            performance.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyterDisplay(s):\n",
    "    clear_output(wait=True)\n",
    "    display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "scores = REINFORCE(env,\n",
    "                   state_space_size=state_space_size,\n",
    "                   action_space_size=action_space_size,\n",
    "                   num_episodes=5000,\n",
    "                   disp=jupyterDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(scores)\n",
    "plt.title(\"Episode scores over training\")\n",
    "plt.xlabel(\"Training episodes\")\n",
    "plt.ylabel(\"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth scores\n",
    "N = 50\n",
    "smooth_scores = np.convolve(scores, np.ones((N,))/N, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 8), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(smooth_scores)\n",
    "plt.title(\"Episode scores over training (smoothed over {} episodes)\".format(N))\n",
    "plt.xlabel(\"Training episodes\")\n",
    "plt.ylabel(\"Score (smoothed over {} episodes)\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
